1. KNN (K-Nearest Neighbors)
A lazy, instance-based algorithm that classifies data based on the majority label of the K closest training samples in feature space.

2. Linear Regression
A supervised learning algorithm that models the linear relationship between input features and a continuous target variable.

3. Logistic Regression
A classification algorithm that models the probability of a binary outcome using a logistic (sigmoid) function.

4. Decision Trees
A tree-like model that splits data into branches based on feature values to make predictions. Easy to interpret and visualize.

5. Random Forests
An ensemble method that builds multiple decision trees and combines their outputs (majority vote or averaging) to improve accuracy and reduce overfitting.

6. Naive Bayes
A probabilistic classifier based on Bayes' theorem with the assumption of feature independence. Simple and fast, works well for text data.

7. PAC (Probably Approximately Correct)
A theoretical framework that describes the learnability of algorithms. Not an algorithm itself, but a foundation to understand algorithm performance guarantees.

8. Perceptron
A basic neural network unit for binary classification. It updates weights based on prediction error and is the foundation of modern deep learning.

9. SVM (Support Vector Machine)
A powerful classifier that finds the optimal hyperplane to separate classes in feature space, with support for linear and non-linear kernels.

10. KMeans
An unsupervised clustering algorithm that partitions data into K clusters by minimizing intra-cluster variance (distance to centroid).
